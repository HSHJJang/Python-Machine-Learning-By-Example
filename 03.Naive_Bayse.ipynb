{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **나이브 베이즈 모델을 이용한 스팸메일 분류기**\n",
    "Calssification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1 분류기 Classification**\n",
    "1. **Binary Classification** (이진 분류기) : **True / False 조건을** 구분한다\n",
    "1. **Multiclass Classification** (다변량 분류) : **다양한 클래스간의 조건을** 구분한다\n",
    "1. **Multi-label Classification** (다중 클래스 레이블 분류) : 다중의 클래스간 **겹치는 조건에서** 구분을 한다\n",
    "\n",
    "<img src=\"https://wikidocs.net/images/page/4291/logreg702.PNG\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2 텍스트 분류기 Classification**\n",
    "1. **긍정/ 부정,  긍정/ 중립/ 부정** 분류기\n",
    "1. **뉴스의 토픽** 분류기 (**class 간 중첩되어** 분류가 가능하다)\n",
    "1. **Named Entity Recognition** (개체명 분류기) : ex) Naive Bayse, Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3  Naive Bayse Classification 개념**\n",
    "1. 확률 기반의 분류기\n",
    "1. **Naive :** 예측을 위한 Token 들이 **Mutually Independent** (상호독립적)을 가정\n",
    "1. **Bayse :** 관찰한 Token이 **클래스 전체 대비, 특정 클래스 속할 확률을 Bayse 기반** 으로 계산\n",
    "\n",
    "> **Naive Bayse 메커니즘**\n",
    "\n",
    "1. 스팸메일과, 정상메일로 구분된 데이터를 사용한다 [download](http://www.aueb.gr/users/ion/data/enron-spam/preprocessed/enron1.tar.gz)\n",
    "1. 단어 **Token을** 대상으로 **스팸여부를** 학습한다.\n",
    "1. Data 추가시 잘못 예측한 결과에 대해 **Laplace Smoothing** 으로 보완한 값을 **Bayse 로 공식을** 수정한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4   Naive Bayse 구현하기**\n",
    "스펨메일 데이터 다운받기 [download](http://www.aueb.gr/users/ion/data/enron-spam/preprocessed/enron1.tar.gz)\n",
    "<br>\n",
    "### **01 enron 메일데이터 살펴보기**\n",
    "1. **Summary.txt** 파일에 저장된 내용 살펴보기\n",
    "1. **정상메일 (3,672개)** 와 **스펨메일 (1,500)개로** 약 1:2의 비율로 구분이 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legitimate\n",
      "----------\n",
      "- Owner: farmer-d\n",
      "- Total number: 3672 emails\n",
      "- Date of first email: 1999-12-10\n",
      "- Date of last email: 2002-01-11\n",
      "- Similars deletion: No\n",
      "- Encoding: No\n",
      "\n",
      "\n",
      "Spam\n",
      "----\n",
      "- Owner: GP\n",
      "- Total number: 1500 emails\n",
      "- Date of first email: 2003-12-18\n",
      "- Date of last email: 2005-09-06\n",
      "- Similars deletion: No\n",
      "- Encoding: No\n",
      "\n",
      "Spam:Legitimate rate = 1:3\n",
      "Total number of emails (legitimate + spam): 5975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 스팸메일 데이터 Summary\n",
    "with open('./data/enron1/Summary.txt', 'r') as f:\n",
    "    summary = f.read()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: mcmullen gas for 11 / 99\n",
      "jackie ,\n",
      "since the inlet to 3 river plant is shut in on 10 / 19 / 99 ( the last day of\n",
      "flow ) :\n",
      "at what meter is the mcmullen gas being diverted to ?\n",
      "at what meter is hpl buying the residue gas ? ( this is the gas from teco ,\n",
      "vastar , vintage , tejones , and swift )\n",
      "i still see active deals at meter 3405 in path manager for teco , vastar ,\n",
      "vintage , tejones , and swift\n",
      "i also see gas scheduled in pops at meter 3404 and 3405 .\n",
      "please advice . we need to resolve this as soon as possible so settlement\n",
      "can send out payments .\n",
      "thanks\n"
     ]
    }
   ],
   "source": [
    "# ham 폴더에 저장된 메일내용 확인 (정상으로 분류된 메일)\n",
    "file_path = './data/enron1/ham/0007.1999-12-14.farmer.ham.txt'\n",
    "with open(file_path, 'r') as infile:\n",
    "    ham_sample = infile.read()\n",
    "print(ham_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: stacey automated system generating 8 k per week parallelogram\n",
      "people are\n",
      "getting rich using this system ! now it ' s your\n",
      "turn !\n",
      "we ' ve\n",
      "cracked the code and will show you . . . .\n",
      "this is the\n",
      "only system that does everything for you , so you can make\n",
      "money\n",
      ". . . . . . . .\n",
      "because your\n",
      "success is . . . completely automated !\n",
      "let me show\n",
      "you how !\n",
      "click\n",
      "here\n",
      "to opt out click here % random _ text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spam 폴더에 저장된 메일내용 확인 (스팸으로 분류된 메일)\n",
    "file_path = './data/enron1/spam/0058.2003-12-21.GP.spam.txt'\n",
    "with open(file_path, 'r') as infile:\n",
    "    spam_sample = infile.read()\n",
    "print(spam_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **02 enron 메일 데이터 분류하기**\n",
    "1. 스펨메일과 정상메일을 레이블을 사용하여 분류한다\n",
    "1. 1 : 스펨메일,  0 : 정상메일\n",
    "1. 분류된 데이터를 전처리 과정을 진행한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob,os\n",
    "# 정상매일은 0, 스펨매일은 1\n",
    "emails, labels = [], []\n",
    "for no, file_path in enumerate(['./data/enron1/ham/','./data/enron1/spam/']):\n",
    "    for filename in glob.glob(os.path.join(file_path, '*.txt')):\n",
    "        with open(filename, 'r', encoding = \"ISO-8859-1\") as infile:\n",
    "            emails.append(infile.read())\n",
    "            labels.append(no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **03 enron 메일 데이터 임베딩**\n",
    "1. Chapter 2 에서 진행한 내용을 바탕으로 전처리 작업을 진행한다\n",
    "1. **숫자와 구두점** 제거, **StopWords** 제거, **표제어 원형** 복원\n",
    "1. 정제된 데이터로 **희소벡터 (Sparse Vector)** 로 임베딩 ex) (**row index, feacture/term index**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fw customer list this one includes more financial counterparties non eol original message from winfree o neal d sent wednesday december am to martin thomas a subject customer list tom attached are eol customer between july and nov broken out by physical and financial the only physical customer i remember non eol are imperial sugar and texas energy i m also checking this list against other non eol deal eric or joe might have done but for now take a look at this o'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import names\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "all_names  = set(names.words())\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 알파벳이 아닌 객체가 포함시 False를 출력\n",
    "def letters_only(astr): \n",
    "    return astr.isalpha()\n",
    "\n",
    "# 표제어 복원작업\n",
    "def clean_text(docs):\n",
    "    cleaned    = [' '.join([lemmatizer.lemmatize(word.lower())\n",
    "                        for word in doc.split()\n",
    "                        if letters_only(word) and word not in all_names])   \n",
    "              for doc in docs]\n",
    "    return cleaned\n",
    "\n",
    "# 사용자 함수를 활용하여 전처리 작업을 진행한다\n",
    "cleaned_emails = clean_text(emails)\n",
    "cleaned_emails[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 Type: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "임베딩의 크기: (5172, 500)\n",
      "0번문장 내용보기: \n",
      "  (0, 248)\t1\n",
      "  (0, 102)\t1\n",
      "  (0, 125)\t1\n",
      "  (0, 435)\t1\n",
      "  (0, 224)\t1\n",
      "  (0, 30)\t1\n",
      "  (0, 447)\t1\n",
      "  (0, 417)\t1\n",
      "  (0, 104)\t1\n",
      "  (0, 482)\t1\n",
      "  (0, 390)\t1\n",
      "  (0, 265)\t1\n",
      "  (0, 307)\t1\n",
      "  (0, 147)\t2\n",
      "  (0, 241)\t3\n",
      "  (0, 94)\t4\n",
      "  (0, 162)\t1\n",
      "CPU times: user 594 ms, sys: 4.05 ms, total: 598 ms\n",
      "Wall time: 597 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 출현빈도가 높은 상위 500개의 Token을 대상으로 임베딩 한다\n",
    "# 희소벡터(Sparse Vector)로 변환 : (row index, feacture/term index)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(stop_words=\"english\", max_features=500)\n",
    "term_docs      = cv.fit_transform(cleaned_emails)\n",
    "print(\"모델의 Type: {}\\n임베딩의 크기: {}\\n0번문장 내용보기: \\n{}\".format(\n",
    "    type(term_docs),\n",
    "    term_docs.shape, # 5,172개 문장을 500개 단어로 생성\n",
    "    term_docs [0]))  # 0번 문장의 단어 Vector 목록을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'access', 'account', 'accounting', 'act', 'action', 'activity']\n",
      "0 : able\n",
      "162 : fw\n",
      "481 : website\n",
      "357 : read\n",
      "125 : energy\n"
     ]
    }
   ],
   "source": [
    "# cv 모델로 인덱스별 단어 Token 내용보기\n",
    "# feature_mapping = cv.vocabulary_       # dict 로 내용출력 (key:value)\n",
    "\n",
    "print(cv.get_feature_names()[:7])\n",
    "feature_names   = cv.get_feature_names() # List 로 내용출력 (인덱스별 value)\n",
    "for indx in [0, 162, 481, 357, 125]:\n",
    "    print(indx, \":\", feature_names[indx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **04-1 Naive Bayse 학습을 위한 준비작업**\n",
    "모델의 학습을 위한 준비작업으로 데이터를 그룹화 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블을 기준으로 데이터를 그룹화 한다\n",
    "# defaultdict : 스팸여부 0,1 Tag 로 Token Index List 생성\n",
    "def get_label_index(labels):\n",
    "    from collections import defaultdict\n",
    "    label_index = defaultdict(list)\n",
    "    for index, label in enumerate(labels):\n",
    "        label_index[label].append(index)\n",
    "    return label_index\n",
    "\n",
    "# 0 ~ 3600 : 정상메일, 3600 ~ 나머지 : 스팸메일\n",
    "label_index = get_label_index(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **04-2 Naive Bayse 위한 사전확률/ 우도값 계산**\n",
    "**사전확률 및 우도값을** 계산하는 함수를 정의한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.7099767981438515, 1: 0.2900232018561485}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 샘플을 활용하여 사전 확률을 계산하는 \n",
    "def get_prior(label_index):\n",
    "    \"\"\" Compute prior based on training samples\n",
    "    Args:    label_index (grouped sample indices by class)\n",
    "    Returns: dictionary, with class label as key, corresponding prior as the value \"\"\"\n",
    "    prior = {label: len(index) for label, index in label_index.items()}\n",
    "    total_count = sum(prior.values())\n",
    "    for label in prior:\n",
    "        prior[label] /= float(total_count)\n",
    "    return prior\n",
    "\n",
    "# 위의 인덱스 데이터를 활용하여 사전확률을 계산한다\n",
    "prior = get_prior(label_index)\n",
    "prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.08580656e-03, 9.57737068e-04, 8.79781725e-04, 8.46372292e-04,\n",
       "       1.00228298e-04, 2.39434267e-04, 1.97115652e-03, 1.34194554e-03,\n",
       "       1.84308703e-03, 8.35235815e-04, 1.08023832e-03, 1.03012417e-03,\n",
       "       1.67047163e-05, 1.50342447e-04, 7.62848711e-04, 8.40804054e-04,\n",
       "       8.12962860e-04, 2.22172727e-03, 1.99342948e-03, 5.01141489e-05])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 확률적 우도값을 계산하는 함수\n",
    "# 위에서 추출한 빈도상위 500개의 단어로 조건부 확률 p(feature|spam) 을 계산\n",
    "def get_likelihood(term_document_matrix, label_index, smoothing=0):\n",
    "    \"\"\" Compute likelihood based on training samples\n",
    "    Args:\n",
    "        term_document_matrix (sparse matrix)\n",
    "        label_index (grouped sample indices by class)\n",
    "        smoothing (integer, additive Laplace smoothing parameter)\n",
    "    Returns: \n",
    "        dictionary, with class as key, corresponding conditional probability P(feature|class) vector as value\n",
    "    \"\"\"\n",
    "    likelihood = {}\n",
    "    for label, index in label_index.items():\n",
    "        likelihood[label] = term_document_matrix[index, :].sum(axis=0) + smoothing\n",
    "        likelihood[label] = np.asarray(likelihood[label])[0]\n",
    "        total_count       = likelihood[label].sum()\n",
    "        likelihood[label] = likelihood[label] / float(total_count)\n",
    "    return likelihood\n",
    "\n",
    "# 라플라스 스무딩시 추가할 값 1\n",
    "smoothing  = 1\n",
    "likelihood = get_likelihood(term_docs, label_index, smoothing)\n",
    "# 0 번(정상메일) 자료 중 1~20번 메일의 우도 값\n",
    "likelihood[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able', 'access', 'account', 'accounting', 'act']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인덱스에 연계된 단어들을 확인\n",
    "feature_names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **04-3 자연 Log 를 활용한 예측함수 구현하기**\n",
    "**사전확률 및 우도값을** 계산하는 함수를 정의한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OverFlow가 발생 가능하므로, 데이터를 Log() 자연로그로 변환 후 덧셈 계산,\n",
    "# 계산이 끝난 뒤, 로그의 역함수 (exp()) 를 활용하여 실수로 변환한다\n",
    "def get_posterior(term_document_matrix, prior, likelihood):\n",
    "    \"\"\" Compute posterior of testing samples, based on prior and likelihood\n",
    "    Args:\n",
    "        term_document_matrix (sparse matrix)\n",
    "        prior (dictionary, with class label as key, corresponding prior as the value)\n",
    "        likelihood (dictionary, with class label as key, corresponding conditional probability vector as value)\n",
    "    Returns:\n",
    "        dictionary, with class label as key, corresponding posterior as value\n",
    "    \"\"\"\n",
    "    num_docs   = term_document_matrix.shape[0]\n",
    "    posteriors = []\n",
    "    for i in range(num_docs):\n",
    "        # posterior is proportional to prior * likelihood\n",
    "        # = exp(log(prior * likelihood))\n",
    "        # = exp(log(prior) + log(likelihood))\n",
    "        posterior = {key: np.log(prior_label) for key, prior_label in prior.items()}\n",
    "        for label, likelihood_label in likelihood.items():\n",
    "            term_document_vector = term_document_matrix.getrow(i)\n",
    "            counts = term_document_vector.data\n",
    "            indices = term_document_vector.indices\n",
    "            for count, index in zip(counts, indices):\n",
    "                posterior[label] += np.log(likelihood_label[index]) * count\n",
    "        # exp(-1000):exp(-999) will cause zero division error,\n",
    "        # however it equates to exp(0):exp(1)\n",
    "        min_log_posterior = min(posterior.values())\n",
    "        for label in posterior:\n",
    "            try:\n",
    "                posterior[label] = np.exp(posterior[label] - min_log_posterior)\n",
    "            except:\n",
    "                # if one's log value is excessively large, assign it infinity\n",
    "                posterior[label] = float('inf')\n",
    "        # normalize so that all sums up to 1\n",
    "        sum_posterior = sum(posterior.values())\n",
    "        for label in posterior:\n",
    "            if posterior[label] == float('inf'):\n",
    "                posterior[label] = 1.0\n",
    "            else:\n",
    "                posterior[label] /= sum_posterior\n",
    "        posteriors.append(posterior.copy())\n",
    "    return posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{0: 0.9815828838777807, 1: 0.018417116122219333},\n",
      " {0: 1.5274461154428757e-06, 1: 0.9999984725538845}]\n"
     ]
    }
   ],
   "source": [
    "# 테스트 메일을 사용하여 알고리즘을 검증해본다\n",
    "emails_test = [\n",
    "    '''Subject: flat screens hello ,\n",
    "    please call or contact regarding the other flat screens requested .\n",
    "    trisha tlapek - eb 3132 b michael sergeev - eb 3132 a\n",
    "    also the sun blocker that was taken away from eb 3131 a .\n",
    "    trisha should two monitors also michael .thanks kevin moore''',\n",
    "    \n",
    "    '''Subject: having problems in bed ? we can help !\n",
    "    cialis allows men to enjoy a fully normal sex life without having to plan the sexual act .\n",
    "    if we let things terrify us , life will not be worth living .\n",
    "    brevity is the soul of lingerie . suspicion always haunts the guilty mind .''']\n",
    "\n",
    "# 검증결과 0번 메일은 0.99로 정상, 1번 메일은 0.99로 스펨메일\n",
    "# 그런데 1번메일의 결과값 총 합아 1을 넘어가냐 ???\n",
    "cleaned_test   = clean_text(emails_test)\n",
    "term_docs_test = cv.transform(cleaned_test)\n",
    "posterior      = get_posterior(term_docs_test, prior, likelihood)\n",
    "from pprint import pprint\n",
    "pprint(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **04-4 학습을 위해 Train / Test 데이터를 나눈다**\n",
    "scikit-learn 모듈 **train_test_split** 을 사용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Length X : 3,465, Y : 3,465\n",
      "Test Data Length  X : 1,707, Y : 1,707\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    cleaned_emails, \n",
    "    labels, \n",
    "    test_size    = 0.33, \n",
    "    random_state = 42)\n",
    "print(\"Train Data Length X : {:,}, Y : {:,}\\nTest Data Length  X : {:,}, Y : {:,}\".format(\n",
    "    len(X_train), len(Y_train), len(X_test), len(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/python/lib/python3.6/site-packages/ipykernel_launcher.py:30: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,707 개의 테스트 데이터(Y_test)의 정확도는: 91.6 %\n"
     ]
    }
   ],
   "source": [
    "# 데이터 Set의 사후 확률을 예측한다\n",
    "term_docs_train = cv.fit_transform(X_train)\n",
    "label_index     = get_label_index(Y_train)\n",
    "prior           = get_prior(label_index)\n",
    "likelihood      = get_likelihood(term_docs_train, label_index, smoothing)\n",
    "\n",
    "# Test / 신규 데이터 Set의 사후확률을 예측한다\n",
    "term_docs_test = cv.transform(X_test)\n",
    "posterior = get_posterior(term_docs_test, prior, likelihood)\n",
    "correct   = 0.0\n",
    "\n",
    "for pred, actual in zip(posterior, Y_test):\n",
    "    if actual == 1:\n",
    "        if pred[1] >= 0.5:\n",
    "            correct += 1\n",
    "    elif pred[0] > 0.5:\n",
    "        correct += 1\n",
    "\n",
    "# dtype 을 128 이상으로 지정할 것\n",
    "# https://stackoverflow.com/questions/40726490/overflow-error-in-pythons-numpy-exp-function/40726641\n",
    "print('{:,} 개의 테스트 데이터(Y_test)의 정확도는: {:.1f} %'.format(\n",
    "    len(Y_test), correct/len(Y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "## **5 Sklearn 을 활용한 Naive Bayse 구현하기**\n",
    "위에서 복잡한 과정을 통해서 내용을 숙달하길 바라며, sklearn으로 위의 내용을 진행해본다\n",
    "### **01 데이터 전처리 및 모델학습**\n",
    "모델을 학습한 뒤 정확도를 측정한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99808489e-01, 1.91511166e-04],\n",
       "       [9.99999772e-01, 2.28176513e-07],\n",
       "       [9.99999223e-01, 7.77402015e-07],\n",
       "       [9.99999724e-01, 2.76311984e-07],\n",
       "       [9.98447799e-01, 1.55220148e-03],\n",
       "       [1.00000000e+00, 2.17331050e-15]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha     = 1.0,  # Laplace Smoothing 값\n",
    "                    fit_prior = True) # Data Set로 학습된 사전확률 사용\n",
    "clf.fit(term_docs_train, Y_train)\n",
    "\n",
    "# 예측 결과값을 계산한다\n",
    "prediction_prob = clf.predict_proba(term_docs_test)\n",
    "prediction_prob[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측한 클래스 값을 바로 계산하여 출력한다\n",
    "# 역치값은 0.5로 0.5보다 크면 1, 작으면 0을 출력\n",
    "prediction = clf.predict(term_docs_test)\n",
    "prediction[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy using MultinomialNB is: 91.6%\n"
     ]
    }
   ],
   "source": [
    "# test 값을 활용하여 모델의 정확도 측정 \n",
    "accuracy = clf.score(term_docs_test, Y_test)\n",
    "print('The accuracy using MultinomialNB is: {0:.1f}%'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### **02 분류기의 성능 평가**\n",
    "**혼동행렬(Confusion Matrix) 분할표로** 예측값을 테스트하여 출력한다\n",
    "\n",
    "<img src=\"https://docs.microsoft.com/ko-kr/azure/machine-learning/studio/media/evaluate-model-performance/6a.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1138,   91],\n",
       "       [  52,  426]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 혼동행렬 값을계산해보자\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_test, prediction, labels=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precesion(정밀도) : 0.824\n",
      "Recall(재현율) : 0.8912\n",
      "f1 score (1) : 0.8563 \n",
      "f1 score (0) : 0.9409\n"
     ]
    }
   ],
   "source": [
    "# f1 Score 를 측정하여 정밀도, 재연율을 계산\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "print(\"\"\"Precesion(정밀도) : {:.4}\\nRecall(재현율) : {:.4}\n",
    "f1 score (1) : {:.4} \\nf1 score (0) : {:.4}\"\"\".format(\n",
    "    precision_score(Y_test, prediction, pos_label=1),\n",
    "    recall_score(Y_test, prediction, pos_label=1),\n",
    "    f1_score(Y_test, prediction, pos_label=1),\n",
    "    f1_score(Y_test, prediction, pos_label=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.94      1229\n",
      "           1       0.82      0.89      0.86       478\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      1707\n",
      "   macro avg       0.89      0.91      0.90      1707\n",
      "weighted avg       0.92      0.92      0.92      1707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 위와 같은내용을 한꺼번에 실행해본다\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test, prediction)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **03 분류기의 성능 평가**\n",
    "1. **정확도**(훈련데이터 적합도) 와 **재현율**(일반화 정도)이 **모두 높은 경우가 없기** 때문에 f1-score를 측정한다\n",
    "1. 하지만 모델의 **평균값과,** 모델의 **f1-score** 둘 다 높은 모델은 없으므로 별도 기준이 필요\n",
    "1. 대표적인 대안으로 **ROC (Receiver Operation Characteristic), AUC (Area Under the Curve)** 가 있다\n",
    "1. 이번 예제에서는 **ROC**를 그려보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.65 ms, sys: 0 ns, total: 2.65 ms\n",
      "Wall time: 2.61 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ROC Curve를 그려보자\n",
    "pos_prob   = prediction_prob[:, 1]\n",
    "thresholds = np.arange(0.0, 1.2, 0.1)\n",
    "true_pos   = [0]*len(thresholds) \n",
    "false_pos  = [0]*len(thresholds)\n",
    "\n",
    "for pred, y in zip(pos_prob, Y_test):\n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        if pred >= threshold:\n",
    "            if y == 1: true_pos[i] += 1\n",
    "            else:      false_pos[i] += 1\n",
    "        else: break\n",
    "\n",
    "# 임계치를 설정하기 위해 양성비율과, 음성 비율을 계산한다\n",
    "# 양성 테스트 샘플이 516개, 음성 테스트 샘플이 1,191개 이다\n",
    "true_pos_rate  = [tp / 516.0 for tp in true_pos]\n",
    "false_pos_rate = [fp / 1191.0 for fp in false_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# ROC Curve 를 출력한다\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# BaseLine을 그린다\n",
    "lw = 2\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.plot(false_pos_rate, true_pos_rate, color = 'darkorange', lw = lw)\n",
    "plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\"); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9629610085418291"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(Y_test, pos_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **6 Sklearn 을 활용한 모델의 튜닟 및 교차검증**\n",
    "1. 모델이 실질적으로 잘 작동하는지 **K-fold 검정을** 적용한다\n",
    "1. **AUC 값의** 측정 : **ROC 커브의** 밑면적을 구한 값으로 **1에 가까울수록** 성능이 좋다.[참고](http://newsight.tistory.com/53)\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 10개의 폴드 생성기로 초기화 후 파라미터 분석을 진행합니다\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "k = 10\n",
    "k_fold = StratifiedKFold(n_splits=k)\n",
    "\n",
    "# 연산을 위해 Numpy 객체로 변환한다\n",
    "cleaned_emails_np = np.array(cleaned_emails)\n",
    "labels_np         = np.array(labels)\n",
    "\n",
    "# 10 폴드 생성기 학습을 위한 파라미터를 정의합니다\n",
    "max_features_option     = [2000, 4000, 8000]   # 가장 많이 사용되는 N개 단어를 선택\n",
    "smoothing_factor_option = [0.5, 1.0, 1.5, 2.0] # Smoothing Parameter : 초기값\n",
    "fit_prior_option        = [True, False]        # 사전 확률을 사용할지 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2000: {0.5: {True: 9.774037510366071, False: 9.77421866978636}, 1.0: {True: 9.759365473087707, False: 9.760037293172216}, 1.5: {True: 9.748388421593019, False: 9.747397943569085}, 2.0: {True: 9.737427906448682, False: 9.735719972949493}}, 4000: {0.5: {True: 9.84587346483434, False: 9.845428291474153}, 1.0: {True: 9.833152396043122, False: 9.83388784405481}, 1.5: {True: 9.823646536745251, False: 9.824236662322791}, 2.0: {True: 9.81472984046124, False: 9.813894113059273}}, 8000: {0.5: {True: 9.885893851439404, False: 9.885203569877188}, 1.0: {True: 9.875745024286223, False: 9.87436441179955}, 1.5: {True: 9.870388826363387, False: 9.87023407574142}, 2.0: {True: 9.865715209493345, False: 9.865261027524385}}}\n",
      "CPU times: user 25.1 s, sys: 1.24 s, total: 26.3 s\n",
      "Wall time: 26.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "auc_record = {}\n",
    "for train_indices, test_indices in k_fold.split(cleaned_emails, labels):\n",
    "    X_train, X_test = cleaned_emails_np[train_indices], cleaned_emails_np[test_indices]\n",
    "    Y_train, Y_test = labels_np[train_indices], labels_np[test_indices]\n",
    "    for max_features in max_features_option:\n",
    "        if max_features not in auc_record:\n",
    "            auc_record[max_features] = {}\n",
    "        cv = CountVectorizer(stop_words=\"english\", max_features=max_features)\n",
    "        term_docs_train = cv.fit_transform(X_train)\n",
    "        term_docs_test  = cv.transform(X_test)\n",
    "        for smoothing_factor in smoothing_factor_option:\n",
    "            if smoothing_factor not in auc_record[max_features]:\n",
    "                auc_record[max_features][smoothing_factor] = {}\n",
    "            for fit_prior in fit_prior_option:\n",
    "                clf = MultinomialNB(alpha=smoothing_factor, fit_prior=fit_prior)\n",
    "                clf.fit(term_docs_train, Y_train)\n",
    "                prediction_prob = clf.predict_proba(term_docs_test)\n",
    "                pos_prob = prediction_prob[:, 1]\n",
    "                auc = roc_auc_score(Y_test, pos_prob)\n",
    "                auc_record[max_features][smoothing_factor][fit_prior] \\\n",
    "                    = auc + auc_record[max_features][smoothing_factor].get(fit_prior, 0.0)\n",
    "\n",
    "print(auc_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max features  smoothing  fit prior  auc\n",
      "       2000      0.5      True    0.9774\n",
      "       2000      0.5      False    0.9774\n",
      "       2000      1.0      True    0.9759\n",
      "       2000      1.0      False    0.9760\n",
      "       2000      1.5      True    0.9748\n",
      "       2000      1.5      False    0.9747\n",
      "       2000      2.0      True    0.9737\n",
      "       2000      2.0      False    0.9736\n",
      "       4000      0.5      True    0.9846\n",
      "       4000      0.5      False    0.9845\n",
      "       4000      1.0      True    0.9833\n",
      "       4000      1.0      False    0.9834\n",
      "       4000      1.5      True    0.9824\n",
      "       4000      1.5      False    0.9824\n",
      "       4000      2.0      True    0.9815\n",
      "       4000      2.0      False    0.9814\n",
      "       8000      0.5      True    0.9886\n",
      "       8000      0.5      False    0.9885\n",
      "       8000      1.0      True    0.9876\n",
      "       8000      1.0      False    0.9874\n",
      "       8000      1.5      True    0.9870\n",
      "       8000      1.5      False    0.9870\n",
      "       8000      2.0      True    0.9866\n",
      "       8000      2.0      False    0.9865\n"
     ]
    }
   ],
   "source": [
    "# 위에서 계산한 결과를 출력하빈다\n",
    "print('max features  smoothing  fit prior  auc')\n",
    "for max_features, max_feature_record in auc_record.items():\n",
    "    for smoothing, smoothing_record in max_feature_record.items():\n",
    "        for fit_prior, auc in smoothing_record.items():\n",
    "            print('       {0}      {1}      {2}    {3:.4f}'.format(max_features, smoothing, fit_prior, auc/k))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
